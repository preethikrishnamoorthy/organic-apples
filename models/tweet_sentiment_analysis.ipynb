{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09cd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import regex as re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91431918",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Tweets.csv\")\n",
    "df = (data [[\"textID\", \"text\", \"sentiment\"]]).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3531d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "#     Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9987cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(df[\"text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29f7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process (x):\n",
    "    if x ==\"positive\": return 1\n",
    "    elif x==\"negative\": return -1\n",
    "    else: return 0\n",
    "\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = np.array(list(map(process, df[\"sentiment\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "480b5c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 20:24:09,864 : INFO : collecting all words and their counts\n",
      "2022-09-10 20:24:09,865 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2022-09-10 20:24:09,866 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-09-10 20:24:09,912 : INFO : PROGRESS: at sentence #10000, processed 686846 words, keeping 99 word types\n",
      "2022-09-10 20:24:09,966 : INFO : PROGRESS: at sentence #20000, processed 1368026 words, keeping 99 word types\n",
      "2022-09-10 20:24:10,005 : INFO : collected 101 word types from a corpus of 1877709 raw words and 27480 sentences\n",
      "2022-09-10 20:24:10,006 : INFO : Creating a fresh vocabulary\n",
      "2022-09-10 20:24:10,007 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 101 unique words (100.0%% of original 101, drops 0)', 'datetime': '2022-09-10T20:24:10.007812', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-09-10 20:24:10,008 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1877709 word corpus (100.0%% of original 1877709, drops 0)', 'datetime': '2022-09-10T20:24:10.008696', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-09-10 20:24:10,009 : INFO : deleting the raw counts dictionary of 101 items\n",
      "2022-09-10 20:24:10,009 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2022-09-10 20:24:10,009 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 439212.4806494596 word corpus (23.4%% of prior 1877709)', 'datetime': '2022-09-10T20:24:10.009695', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-09-10 20:24:10,011 : INFO : estimated required memory for 101 words and 100 dimensions: 131300 bytes\n",
      "2022-09-10 20:24:10,012 : INFO : resetting layer weights\n",
      "2022-09-10 20:24:10,013 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-09-10T20:24:10.013695', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2022-09-10 20:24:10,013 : INFO : Word2Vec lifecycle event {'msg': 'training model with 16 workers on 101 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2022-09-10T20:24:10.013695', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-09-10 20:24:10,277 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2022-09-10 20:24:10,279 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2022-09-10 20:24:10,284 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2022-09-10 20:24:10,286 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2022-09-10 20:24:10,286 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2022-09-10 20:24:10,288 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2022-09-10 20:24:10,291 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-09-10 20:24:10,293 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-09-10 20:24:10,295 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-09-10 20:24:10,296 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-09-10 20:24:10,296 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-09-10 20:24:10,297 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-09-10 20:24:10,298 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-09-10 20:24:10,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-09-10 20:24:10,299 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-09-10 20:24:10,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-09-10 20:24:10,300 : INFO : EPOCH - 1 : training on 1877709 raw words (439078 effective words) took 0.3s, 1590363 effective words/s\n",
      "2022-09-10 20:24:10,593 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2022-09-10 20:24:10,598 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2022-09-10 20:24:10,599 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2022-09-10 20:24:10,601 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2022-09-10 20:24:10,603 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2022-09-10 20:24:10,606 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2022-09-10 20:24:10,609 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-09-10 20:24:10,611 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-09-10 20:24:10,614 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-09-10 20:24:10,614 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-09-10 20:24:10,615 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-09-10 20:24:10,616 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-09-10 20:24:10,617 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-09-10 20:24:10,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-09-10 20:24:10,618 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-09-10 20:24:10,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-09-10 20:24:10,619 : INFO : EPOCH - 2 : training on 1877709 raw words (438745 effective words) took 0.3s, 1425035 effective words/s\n",
      "2022-09-10 20:24:10,946 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2022-09-10 20:24:10,947 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2022-09-10 20:24:10,952 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2022-09-10 20:24:10,954 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2022-09-10 20:24:10,958 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2022-09-10 20:24:10,959 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2022-09-10 20:24:10,961 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-09-10 20:24:10,966 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-09-10 20:24:10,968 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-09-10 20:24:10,969 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-09-10 20:24:10,972 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-09-10 20:24:10,974 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-09-10 20:24:10,975 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-09-10 20:24:10,976 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-09-10 20:24:10,978 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-09-10 20:24:10,979 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-09-10 20:24:10,981 : INFO : EPOCH - 3 : training on 1877709 raw words (438940 effective words) took 0.3s, 1260948 effective words/s\n",
      "2022-09-10 20:24:11,330 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2022-09-10 20:24:11,334 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2022-09-10 20:24:11,340 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2022-09-10 20:24:11,341 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2022-09-10 20:24:11,342 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2022-09-10 20:24:11,344 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2022-09-10 20:24:11,347 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-10 20:24:11,350 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-09-10 20:24:11,351 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-09-10 20:24:11,351 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-09-10 20:24:11,352 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-09-10 20:24:11,353 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-09-10 20:24:11,354 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-09-10 20:24:11,354 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-09-10 20:24:11,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-09-10 20:24:11,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-09-10 20:24:11,356 : INFO : EPOCH - 4 : training on 1877709 raw words (439175 effective words) took 0.4s, 1224006 effective words/s\n",
      "2022-09-10 20:24:11,664 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2022-09-10 20:24:11,667 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2022-09-10 20:24:11,668 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2022-09-10 20:24:11,669 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2022-09-10 20:24:11,673 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2022-09-10 20:24:11,674 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2022-09-10 20:24:11,676 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-09-10 20:24:11,677 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-09-10 20:24:11,682 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-09-10 20:24:11,683 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-09-10 20:24:11,685 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-09-10 20:24:11,686 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-09-10 20:24:11,686 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-09-10 20:24:11,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-09-10 20:24:11,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-09-10 20:24:11,689 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-09-10 20:24:11,689 : INFO : EPOCH - 5 : training on 1877709 raw words (438746 effective words) took 0.3s, 1365499 effective words/s\n",
      "2022-09-10 20:24:11,690 : INFO : Word2Vec lifecycle event {'msg': 'training on 9388545 raw words (2194684 effective words) took 1.7s, 1310242 effective words/s', 'datetime': '2022-09-10T20:24:11.690292', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-09-10 20:24:11,690 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=101, vector_size=100, alpha=0.025)', 'datetime': '2022-09-10T20:24:11.690292', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=101, vector_size=100, alpha=0.025)\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 19\n",
      "  73  1 48]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 62 94\n",
      "   7 91 10]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "word2vec_model = Word2Vec(X, window=3, min_count=1, workers=16)\n",
    "print(word2vec_model)\n",
    "\n",
    "token = Tokenizer(101)\n",
    "token.fit_on_texts(X)\n",
    "text = token.texts_to_sequences(X)\n",
    "text = pad_sequences(text, 75)\n",
    "print(text[:2])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13a47954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 19\n",
      "  73  1 48]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 62 94\n",
      "   7 91 10]]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e208872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deeb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = (tokenizer.texts_to_sequences([\"egererhe\"]))\n",
    "w = pad_sequences(w, padding='post', maxlen=maxlen)[0]\n",
    "w\n",
    "#Predict Output\n",
    "predicted= model.predict([w]) # 0:Overcast, 2:Mild\n",
    "print( \"Predicted Value:\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dadd9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyper parameters\n",
    "batch_size = 32\n",
    "embedding_dims = 100 #Length of the token vectors\n",
    "filters = 250 #number of filters in your Convnet\n",
    "kernel_size = 3 # a window size of 3 tokens\n",
    "hidden_dims = 250 #number of neurons at the normal feedforward NN\n",
    "epochs = 2\n",
    "maxlen=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6481f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(word2vec_model.wv.get_keras_embedding(True))\n",
    "model.add(Conv1D(filters,kernel_size,padding = 'valid' , activation = 'relu',strides = 1 , input_shape = (maxlen,embedding_dims)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#GlobalMaxPooling1D(n) default = 2.\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51aaabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_6\" is incompatible with the layer: expected shape=(None, 100, 100), found shape=(32, 75)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7bdh3y8p.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\pkris\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_6\" is incompatible with the layer: expected shape=(None, 100, 100), found shape=(32, 75)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train,y_train,batch_size = batch_size,epochs = epochs , validation_data = (X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b66052156ab8f78f561de088d9fbc54bdecac7e8014a9ce2aa7fb5f353827e4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
